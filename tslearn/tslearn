#!/usr/bin/env python
'''
Authors: Jeff Adrion
'''


from tslearn.imports import *
from tslearn.helpers import *
from tslearn.networks import *
from tslearn.simulator import *
from tslearn.treesBatchGenerator import *

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--outDir',dest='outDir',help='Directory for all output',default=None)
    parser.add_argument('--nEpochs',dest='nEpochs',
            help='Maximum number of epochs to train (EarlyStopping is implemented for validation accuracy)', type=int, default=100)
    parser.add_argument('--nValSteps',dest='nValSteps',help='Number of validation steps', type=int, default=20)
    parser.add_argument('--gpuID',dest='gpuID',help='Identifier specifying which GPU to use', type=int, default=0)
    parser.add_argument("--seed",dest="seed",help="random seed",type=int,default=12345)
    parser.add_argument("--nProc",dest="nProc",help="number of cores to use (default uses all available)",type=int,default=None)
    args = parser.parse_args()

    # set seed
    if args.seed:
        os.environ['PYTHONHASHSEED']=str(args.seed)
        random.seed(args.seed)
        np.random.seed(args.seed)
        os.environ['TF_DETERMINISTIC_OPS'] = '1'
        os.environ['TF_CUDNN_DETERMINISTIC'] = '1'
    
    # set number of cores to use
    if args.nProc:
        nProc = args.nProc
    else:
        nProc=mp.cpu_count()

    # save cli command
    cli_cmd = sys.argv

    # set cwd
    if args.outDir:
        cwd = args.outDir
    else:
        cwd = os.getcwd()
    
    # make directory tree
    sets = ["train","vali","test"]
    for subset in ("trees","results"):
        subDir = os.path.join(cwd, subset)
        if not os.path.exists(subDir):
            os.mkdir(subDir)
    treeDir = os.path.join(cwd,"trees")
    for subset in sets:
        subDir = os.path.join(treeDir,subset)
        if not os.path.exists(subDir):
            os.mkdir(subDir)

    
    ##########################SIMULATE##########################
    # define params for data generation
    dg_params = {
        'seed':args.seed,
        'N': 10,
        'Ne':10000,
        'priorLowsRho':0.0,
        'priorHighsRho':1e-6,
        'priorLowsMu':1e-8,
        'priorHighsMu':1e-7,
        'ChromosomeLength':1e3
              }
    
    max_table_size = np.zeros(4) # corresponds to [max_num_nodes, edges, sites, mutations]
    numReps = [20000,2000,1000]

    # simulate and produce trees
    for i, subset in enumerate(sets):
        data_generator = Simulator(**dg_params)
        direc = os.path.join(treeDir,subset)
        table_lens = data_generator.simulateAndProduceTrees(numReps=numReps[i],
                direc=direc,simulator="msprime",nProc=nProc)
        if subset != "test":
            max_table_size = np.maximum(max_table_size, table_lens)
    
    # initialize universal batch generator params
    train_batch_params = {
        'targetNormalization':"zscore",
        'batchSize': 64,
        'frameWidth': 0,
        'center':True,
        'shuffleExamples':True,
        'maxTsTableSize':max_table_size
              }
    vali_batch_params = copy.deepcopy(train_batch_params)
    test_batch_params = copy.deepcopy(train_batch_params)
    
    # define set-specific parameters
    targets = ["mu","rho"]
    params = [train_batch_params, vali_batch_params, test_batch_params]
    for i, subset in enumerate(sets):
        # universal params    
        params[i]['treesDirectory'] = os.path.join(treeDir,subset)
        params[i]['numReps'] = numReps[i]
        info = pickle.load(open(os.path.join(os.path.join(treeDir, subset),"info.p"), "rb"))
        params[i]['rawTargets'] = [info[target] for target in targets]
        
        # test specific params
        if subset == "test":
            params[i]['batchSize'] = numReps[i]
            params[i]['shuffleExamples'] = False
        
        #False write batch parameters
        batchParsFILE=os.path.join(treeDir,"{}_batch_params.p".format(subset))
        with open(batchParsFILE, "wb") as fOUT:
            pickle.dump(params[i],fOUT)

    ## initialize batch generators
    train_batch_gen = treesBatchGenerator(**train_batch_params)
    vali_batch_gen = treesBatchGenerator(**vali_batch_params)
    test_batch_gen = treesBatchGenerator(**test_batch_params)
    
    
    ##########################TRAIN##########################
    ## define model files
    modelFile = os.path.join(cwd,"results","model.json")
    weightsFile = os.path.join(cwd,"results","weights.h5")
    resultsFile = os.path.join(cwd,"results","model.results")
    resultsFig = os.path.join(cwd,"results","results.pdf")

    # train model
    train_model(ModelFuncPointer=DENSE1,
            ModelName="DENSE1",
            targetLabels=targets,
            TrainGenerator=train_batch_gen,
            ValidationGenerator=vali_batch_gen,
            TestGenerator=test_batch_gen,
            resultsFile=resultsFile,
            network=[modelFile,weightsFile],
            numEpochs=args.nEpochs,
            validationSteps=args.nValSteps,
            nProc=1,
            gpuID=args.gpuID)
    
    ## plot the results
    plotResults(resultsFile=resultsFile,saveas=resultsFig)


if __name__ == "__main__":
    main()
